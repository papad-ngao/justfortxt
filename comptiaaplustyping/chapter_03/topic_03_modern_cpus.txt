[1001]
Modern CPUs
CPU manufacturers have achieved stunning progress with microprocessors since the days of Intel 8088, and the rate of change doesn't show any signs of slowing. At the core, though, today's CPU's function similarly to the processors of your forefathers. The arithmetic logic unit (ALU)--that's the Man in the Box--still crunches numbers many billions of times per second. CPUs rely on memory to feed them lines of programming as quicly as possible.
This section brings the CPU into the present. We'll first look at the models you can buy today, and then we'll turn to essential improvements in technology you should understand.
Developers
When IBM awarded Intel the contract to provide the CPUs for its new IBM PC back in 1980, it established for Intel a virtual monopoly on all PC CPUs. The ohter home-computer CPU makes of the time faded away: MOS Technology, Zilog, Motoloa--no one could compete directly with Intel. Over time, other competitors have risen to challenge Intel's market-segment share dominance. A company called Advaned Micro Devices (AMD) began to make clones of Intel CPUs, creating an interesting and rather cutthroad competition with Intel that lasts to this day.
NOTE
The ever-growing selection of mobile devices, such as Apple iPhone and iPad and most Android devices, use a CPU architecture develoepd by ARM holdings, called ARM. ARM-based processors use a simpler, more energy-efficient design, the reduced instruction set computing (RISC) architecture. They can't match the raw power of Intel and AMD complex instruction set computing (CISC) chips, but the savings in cost and battery life make ARM-based processors ideal for mobile devices. (Note that the clear distinction between RISC and CISC processors has blurred. Each design today borrows features of the other design to increase efficiency.)
ARM Holdings designs ARM CPUs, but doesn't manufacture them. Many other companies--most notably, Qualcomm--license the design and manufacture their own versions.
Intel
Intel Corporation thoroughly dominates the personal market with its CPUs and motherboard support chips. At nearly every step in the evolution of PC, Intel has led the way with technological advances and surprising flexibility for such a huge corporation. Intel CPUs--and more specifically, their instruction sets--define the personal computer. Intel currently proces a dozen or so models of CPU for both desktop and portable computers. Most of Intel's desktop and laptop processors are sold under the Core, Pentium, and Celeron brands. Their high-end server chips are called Xeon.
AMD
AMD makes superb CPUs for the PC market and provides competition that keeps Intel on its toes. Like Intel, AMD doesn't just make CPUs, but their CPU business is certainly the part that the public notices. AMD has made CPUs that clond the function of Intel CPus. If Intel invented teh CPU used in the original IBM PC, how could AMD make clone of CPUs without getting sued? Chipmakers have a habit of exchanging technologies through cross-licensed agreement, giving AMD the right to copy certain types of CPUs.
The trouble started with the Intel 8088. Intel needed AMD's help to supply enough CPUs to satisfy IBM's demands. But after a few years, Intel had grown tremendously and no longer wanted AMD to make CPUs. AMD said, "Too bad. See this agreement you signed?" Throughout the 1980s and into 1990s, AMD made pin-for-pin identical CPUs that matched the intel lines of CPUs. You could yank an intel CPU out of a system and snap in an AMD CPU--no problem!
In January 1995, after many years of legal wrangling, Intel and AMD settled and decided to end the licensing agreements. As a result of this settlement, AMD chips are no longer compatible with sockets or motherboards made for Intel CPUs--even though in some cases the chips look similar. Today, if you want to use an AMD CPU, you must purchase as motherboard designed for AMD CPUs. If you want to use an Intel CPU, you must purchase a motherboard designed for Intel CPUs. You have a choice: Intel or AMD.
Model Names
Intel and AMD differentiate producte lines by using different product names, and these names have changed over the years. For a long time, Intel used Pentium for its flagship model, just adding model numbers to show successive generations--Pentium, Pentium II, Pentium III, and so on. AMD used the Athlon brand in similar fashion.
Most discussion on PC CPUs focus on four end-product lines: desktop PC, budget PC, portable PC, and server computers. Below displays many of the current Intel and AMD product lines and names:
- Market: Enthusiast	Intel: Core i7/i9	AMD: Ryzen, Ryzen Threadripper
- Market: Mainstream desktop	Intel: Core i7/i5/i3	AMD: A-Series Pro, Ryzen
- Market: Budget desktop	Intel: Pentium, Celeron	AMD: A-Series, FX
- Market: Portable/Mobile	Intel: Core i7/i5/i3 (mobile), Mobile Celeron	AMD: A-Series
- Market: Server	Intel: Xeon	AMD: Opteron, EPYC
- Market: Workstation	Intel: Xeon	AMD: Ryzen Pro, Ryzen Threadripper
Microarchitecture
Intel and AMD continually develop faster, smarter, and generally more capable CPUs. In general, each company comes with a major new design, called a microarchitecture, about every three years. They try to minimize the number of model names in use, however, most likely for marketing purposes. This means that they release CPUs labeled as the same model, but the CPUs inside can be very different from earlier versions of that model. Both companies used code names to keep track of different variations within models. As a tech, you need to know both the models and code names to be able to make proper recommendations for your clients. One example illustrates the need: the Intel Core i7, where ones code name is "Sandy Bridge", and another, which is also Core i7, but its code name is "Coffee Lake", which is newer and better.
Intel released the first Core i7 in the summer of 2008. By spring of 2012, the original microarchitecture--code named Nahalem--had gone through five variations, none of which worked on motherboards designed for one of the other variations. Plus, in 2011, Intel introduced the Sandy Bridge version of the Core i7 that eventually had two desktop versions and a mobile version, all of which used still other sockets. Just about every year since then has been a new Core i7 based on improved architectures with different code names such as Ivy Bridge, Haswell, Broadwell, and so on. (And I'm simplifying the variations here.)
NOTE
The processor numbers helps a lot when comparing processor once you decode the meanings. We need to cover more about modern processors before introducing processor numbers. Look for more information in the upcoming section, "Deciphering Processor Numbers."
At this point, a lot of new techs throw their hands in the air, How do you keep up? How do you know which CPU will give your customer the best value for his or her money and provide the right computing firepower for his or her needs? Simply put, you need to research efficiently.
Your first stop should be the manufacturers' websites. Both companies put out a lot of information on their products.
- www.intel.com
- www.amd.com
You can also find many high-quality tech Web sites devoted to reporting on the latest CPUs. When a client needs an upgrade, surf the Web for recent articles and make comparisons. Because you'll understand the underlying technology from your CompTIA A+ studies, you'll be able to follow the conversations with confidence. Here's a list of some of the sites I use:
- https://arstechnica.com
- www.anandtech.com
- www.tomshardware.com
- www.bit-tech.net
Finally, you can find great, exhaustive articles on all things tech at Wikipedia:
- www.wikipedia.org
NOTE
Wikipedia is a user-generated, self-regulated resource. I've found it to be accurate on technical issues the majority of the time, but you should always check other references as well. Nicely, most article authors on the side provide their sources through footnotes. You can often use the Wikipedia articles as jumping-off points for deeper searches.
Desktop Versus Mobile
Mobile Devices, such as portable computers, have needs that differ from those of desktop computers, notably the need to consume as little electricity as possible. This helps in two ways: extending battery charge and creating less heat.
Both Intel and AMD have engineers devoted to making excellent mobile versions of their CPUs that sport advanced energy-saving features. These mobile CPUs consume much less power than their desktop counterparts. They also run in very low power mode and scale up automatically if the user demands more power from the CPU. If you're surfing the Web at an airport terminal, the CPU doesn't draw too much power. When you switch to playing an action game, the CPU kicks into gear. Saving energy by making the CPU run more slowly when demand is light is generically called throttling. (which means desktop chip and mobile chip can both be Core i5, but the one on desktop may be more powerful, and another on mobile device consume less electricity)
Unfortunately this picture gets more complicated when you throw in heat. Because most portable and mobile computing devices are very compact, they can't dissipate heat as quickly as a well-cooled desktop system. Mobile CPUs can scale up to handle demanding tasks, but they'll start accumulating heat quickly. As this heat nears levels that could damage the CPU, it will engage in thermal throttling to protect itself. A system trying to do demanding work with only a fraction of its full power available may grind to a halt!
NOTE
The industry describes how much heat a busy CPU generates with a figure (measured in watts) called its thermal design power (TDP). The TDP can give you a rough idea of how much energy a CPU draws and what kind of cooling it will need. It can also help you select more efficient CPUs.
TDP has been trending down over time (especially in recent years), but it may help to have a sense of what these values look like in the real world. The CPUs in a smartphone or tablet typically have a TDP from 2 to 15 watts, laptop CPUs range from 7 to 65 watts, and desktop CPUs tend to range from 50 to 140 watts.
Many of the technologies developed for mobile processors migrate back into their more power-hungry desktop siblings. That's a bonus for the planet (and maybe your power bill).
Technology
Although microprocessors today will server the same function as the venerable 8088--crunching numbers--they do so far more efficiently. Engineers have altered, enhanced, and improved CPUs in a number of ways. This section looks at eight features:
- Clock multipliers.
- 64-bit processing
- Virtualization support
- Parallel execution
- Multicore processing
- Integrated memory controller (IMC)
- Integrated graphics processing unit (GPU)
- Security
Clock Multipliers
All modern CPUs run at some multiple of the system clock speed. The system bus on my Ryzen 7 machine, for example, runs at 100 MHz. The clock multiplier goes up to x32 at full load to support the 3.2 GHz maximum speed. Originally, CPUs ran at the speed of the bus, but engineers early on realized the CPU was the only thing doing any work much of the time. If the engineers could speed up just the internal operations of the CPU and not anything else, they could speed up the whole computing process. There's a nifty program called "CPU-Z" that can display my CPU details. Note that all I'm doing is typing at the moment, so the CPU has dropped the clock multiplier down to x15.5 and the CPU core speed is only 1546 MHz, and it's apparent that the clock speed, multiplier, and bus speed of a Ryzen 7 processor as shown in CPU-Z, hardly breaking a sweat (Core Speed: 1546.86 MHz, Multiplier: x 15.5, Bus Speed: 99.80 MHz)
*Try This!* CPU-Z
Imagine a scenario where you're dumped into an office full of unfamiliar PCs. There's no documentation about the systems at all, so your boss tells you to get cracking and find out as much as possible about each PC ASAP. Try this! Download a copy of the very popular and free CPU-Z utility from www.cpuid.com. CPU-Z gives you every piece of information you'll ever want to know about a CPU. Copy it to a thumb drive, then insert it into a bunch of different computers. (Ask permission, of course!) What kinds of processors do you find in your neighbors' computers? What can you tell about different capabilities?
The clock speed and the multiplier on early clock-multiplying systems had to be manually configured via jumpers or dual in-line package (DIP) switches on the motherboard. Today's CPUs report to the motherboard through a function called CPUID (CPU identifier), and the speed and multiplier are set automatically. (You can manually override this automatic setup on many motherboards. (as known as "Overclocking").
64-Bit Processing
Over successive generations of microprocessors, engineers have upgraded many physical features of CPUs. The EDB gradually increased in size, from 8- to 16- to 32- to 64-bits wide. The address bus similarly jumped, going from 20- to 24- to 32-bits wide (where it stayed for a decade).
The technological features changed as well. Engineers added new and improved registers, for example, that used fancy names like multimedia extensions (MMX) and Streaming SIMD Extensions (SSE). A mighty shift started several years ago and continues to evolve; the move to 64-bit computing.
Most new CPUs support 64-bit processing, meaning they can run a compatible 64-bit operating system, such as Windows 10, and 64-bit applications. They also support 32-bit processing for 32-bit operating systems, such as some Linux distributions, and 32-bit applications. The general-purpose registers also make the move up to 64-bit. The primary benefit to moving to 64-bit computing is that modern systems can support much more than the 4 GB of memory supported with 32-bit processing.
With a 64-bit address bus, CPUs can address 2^64 bytes of memory, or more precisely, 18,446,774,073,709,551,616 bytes of memory--that's a lot of RAM! This number is so bit that gigabytes and terabytes are no longer convenient, so we now go to an exabyte (2^60), abbreviated as EB. A 64-bit address bus can address 16 EB of RAM.
In practical terms, 64-bit computing greatly enhances the performance of programs that work with large files, such as video editing applications. You'll see a profound improvement moving from 4 GB to 8 GB or 16 GB of RAM with such programs.
EXAM TIP
The primary benefit of 64-bit computing is to support more than 4 GB memory, the limit with 32-bit processing.
x86 CPUs from the early days can be lumped together as x86 CPUs. because they used an instruction set that built upon the earliest Intel CPU architecture. The Intel Core 2 Duo, for example, could run a program written for an ancient 80836 processor that was in fashion in the early 1990s.
x64: When the 64-bit CPUs went mainstream, marketing folks needed some way to mark applications, operating systems, and so son such that consumers could quickly tell the difference between something compatible with their system or something not compatible. Since you generally cannot return software after you open it, this is a big deal. The marketing folks went with x64, and that created a mess.
x86-64: The earlier 32-bit stuff had been marketed as x86, not x 32, so now we have x86 (old, 32-bit stuff) versus x64 (new, 64-bit stuff). It's not pretty, but do you get the difference? To make matters even worse, however, x64 processors quite happily handle x86 code and are, by definition, x86 processors too! It's common to marry the two terms and describe current 64-bit CPUs as x86-64 processors.
Virtualization Support
Intel and AMD have built in support for running more than one operating system at a time, a process called virtualization. Virtualization is very cool, but let's skip the detail about that for now. The key issue from a CPU standpoint is that virtualization used to work entirely through software. Programmers had to write a ton of code to enable a CPU--that was designed to run one OS at a time--to run more than one OS at the same time. Think about the issues involved. How does the memory get allocated, for example, or how does the CPU know which OS to update when you type something or click an icon? With hardware-based virtualization support, CPUs took a lot of the burden off the programmers and made virtualization a whole lot easier.
EXAM TIP
The CompTIA A+ 1001 objectives refer to virtualization support as the virtual technology CPU feature.
Parallel Execution
Modern CPUs can process multiple commands and parts of commands in parallel, known as parallel execution. Early processors had to do everything in a strict, linear fashion. The CPUs accomplish this parallelism through multiple pipelines, dedicated cache, and the capability to work with multiple threads or programs at one time. To understand the mighty leap in efficiency gained from parallel execution, you need insight into the processing stages.
Pipelining
To get a command from the data bus, do the calculation, and then send the answer back out onto the data bus, a CPU takes at least four steps (each of these steps is called a stage):
1. Fetch: Get the data from the EDB.
2. Decode: Figure out what type of command needs to be executed
3. Execute: Perform the calculation.
4. Write: Send the data back onto the EDB.
Smart, discrete circuits inside the CPU handle each of these stages. In early CPUs, when a command was placed on the data bus, each stage did its job and the CPU handed back the answer before starting the next command, requiring at least four clock cycles to process a command. In every clock cycle, three of the four circuits sat idle. Today, the circuits are organized in a conveyer-belt fashion called a pipeline. With pipelining, each stage does its job with each clock-cycle pulse, creating a much more efficient process. The CPU has multiple circuits doing multiple jobs, so let's add pipelining to the Man in the Box analogy. Now it's Men in the Box!
Pipelines keep every stage of the processor busy on every click of the clock, making a CPU run more efficiently without increasing the clock speed. Note that at this point, the CPU has four stages: fetch, decode, execute, and write--a four-stage pipeline. No CPU ever made has fewer than four stages, but advancements in caching (see "Cache", next") have increased the number of stages over the years. Current CPU pipelines contain many more stages, up to 20 in some cases.
Pipelining isn't perfect. Sometimes a stage hits a complex command that requires more than one clock cycle, forcing the pipelines to stop. Your CPU tries to avoid these stops, or pipeline stalls. The decode stage tends to cause the most pipeline stalls; certain commands are complex and therefore harder to decode than other commands. Current processors use multiple decode stages to reduce the chance of pipeline stalls due to complex decoding.
The inside of the CPU is composed of multiple chunks of circuitry to handle the many types of calculations your PC needs to do. For example, one part, the arithmetic logic unit (ALU) (or integer unit), handles integer math: basic math for number with no decimal point. A perfect example of integer math is 2 + 3 = 5. The typical CPU spends most of its work doing integer math. CPUs also have special circuitry to handle complex numbers, called the floating point unit (FPU). With a single pipeline, only the ALU or the FPU worked at any execution stage. Worse yet, floating point calculation often took many, many clock cycles to execute, forcing the CPU to stall the pipeline until the FPU finished executing the complex command. Current CPUs offer multiple pipelines to keep the processing going.
Cache
When you send a program to the CPU, you run lots of little programs all at the same time. Okay, let's be fair here: you didn't run all these little programs--you just started your Web browser or some other program. The moment you double-clicked that icon, Windows started sending many programs to the CPU. Each of these programs breaks down into some number of little pieces, called threads, and data. Each thread is a series of instructions designed to do a particular job with the data.
Modern CPUs don't execute instructions sequentially--first doing step 1, then step 2, and so on--but rather process all kinds of instructions. Most applications have certain instructions and data that get reused, sometimes many times.
Pipelining CPUs work fantastically well as long as the pipelines stay filled with instructions. Because the CPU runs faster than the RAM can supply it with code, you'll always get pipeline stalls--called wait states--because the RAM can't keep up with the CPU. To reduce wait states, CPUs some with built-in, very high-speed RAM called static RAM (SRAM). This SRAM preloads as many instructions as possible and keeps copies of already run instructions and data in case the CPU needs to work on them again. SRAM used in this function is called cache. (with the men-in-the box analogy (many man), with RAM, he could say "We're ready for the next line of code... oh look, it's right here in our cache! Good thing, too, because going all the way out to RAM would take forever!")
The SRAM cache inside the early CPUs was tiny, only about 16 KB, but it improved performance tremendously. In fact, it helped so much that many motherboard makers began adding a cache directly to the motherboards. These caches were much larger, usually around 128 to 512 KB. When the CPU looked for a line of code, it first went to the built-in cache; if the code wasn't there, the CPU went to the cache on the motherboard. The cache on the CPU was called the L1 cache because it was the one the CPU first tried to use. The cache on the motherboard was called L2 cache, not because it was on the motherboard, but because it was the second cache the CPU checked.
Eventually, engineers took this cache concept even further and added the L2 cache onto the CPU package, Many modern CPUs include three caches: an L1, an L2, and an L3 cache. (such as in Ryzen 7 processor)
The L2 cache on the early CPUs that had L2 cache included on the CPU package rat at a slower clock speed than the L1 cache. The L1 cache was in the CPU and thus ran at the speed of the CPU. The L2 cache connected to the CPU via a tiny set of wires on the CPU package. The first L2 caches ran at half the speed of the CPU.
The inclusion of L2 cache on the chip gave rise to some new terms to describe the connections between the CPU, MCC, RAM, and L2 cache. The address bus and external data bus (connecting the CPU, MCC, and RAM) were lumped into a single term called a frontside bus, and the connection between the CPU and the L2 cache became known as the backside bus. (These terms don't apply well to current computers, so they have fallen out of use. See the "Integrated Memory Controller" section later in this chapter.)
Put simply: RAM-Frontside bus-MCC // Cache-Backside bus-CPU
NOTE
To keep up with faster processors, motherboard manufacturers began to double and even quadruple the throughput of the frontside bus. Techs sometimes refer to these as double-pumped and quad-pumped frontside buses.
EXAM TIP
Typically, the CompTIA A+ exams expect you to know that L1 cache will be the smallest and fastest cache; L2 will be bigger and slower than L1; and L3 will be the biggest and slowest cache. (This is not completely true anymore, with L1 and L2 running the same speed in many CPUs, but it is how it will appear on the exams.)
Multithreading
At the peak of the single-CPU 32-bit computing days, Intel released a CPU called the Pentium 4 that took parallelism to the next step with Hyper-Threading. Hyper-Threading enabled the Pentium 4 to run multiple threads at the same time, what's generically called simultaneous multithreading, effectively turning the CPU into two CPUs on the chip--with a catch.
If you look in a Task Manager in the "Performance" tab in an ancient Windows XP computer on a system running a Hyper-Thread Pentium 4, you can see how the CPU box is broken into two groups--Windows thinks this one on CPU is two CPUs.
Multithreading enhances a CPU's efficiency but with a couple of limitations. First, the operating system and the application must be designed to take advantage of the feature. Second, although the CPU simulates the actions of a second processor, it doesn't double the processing power because the main execution resources are not duplicated.
Multicore Processing
Microarchitecture hit a plateau back in 2002 when CPU clock speeds hit a practical limit of roughly 4 GHz, motivating the CPU makers to find new ways to get more processing power for CPUs. Although Intel and AMD had different opinions about 64-bit CPUs, both decided at virtually the same time to move beyond the single-core CPU and combine two CPUs (or cores) into a single chip, creating a dual-core architecture. A dual-core CPU has two execution units--two sets of pipelines--but the two sets of pipelines share caches and RAM. A single-core CPU has only one set of everything.
Today, multicore CPUs--with four, six, or eight cores--are common. Higher-end CPUs have up to 32 cores! With each generation of multicore CPU, both Intel and AMD have tinkered with the mixture of how to allocate the cache among the cores. And if you look at the cache breakdown of a Haswell-based Core i7 on CPU-Z, you would see:
- L1 D-Cache, Size: 32 KBytes (x4), Descriptor: 8-way set associative, 64-byte line size
- L1 I-Cache, Size: 32 KBytes (x4), Descriptor: 8-way set associative, 64-byte line size
- L2 Cache, Size: 256 KBytes (x4), Descriptor: 8-way set associative, 64-byte line size
- L3 Cache, Size: 8 MBytes, Descriptor: 16-way set associative, 64-byte line size
And this also reveals specific details about how this Intel CPU works with cache. The Core i7 has L1, L2, and L3 caches of 32 KB, 256 KB, and 8 MB, respectively. (The L1 cache divides into 32 KB to handle data--the D-Cache--and another 32 KB for instructions--the I-Cache). Each core has dedicated L1 and L2 caches. (You can tell this by the x4 just after the capacity listing). All six cores share the giant L3 cache. That pool of memory enables the cores to communicate and work together without having to access the radically slower main system RAM as much.
CPU manufacturers engineered the cores in multicore CPUs to divide up work independently of the OS, known as multicore processing. This differs from Hyper-Threading, where the OS and applications must be written specifically to handle the multiple threads. Note that even with multicore processors, applications must be modified or optimized for this parallelism to have a huge impact on performance.
Because one great technology advancement isn't enough, both Intel and AMD make multicore CPUs that incorporate Hyper-Threading as well. The Intel Core i9-7960X, for example, sports 16 cores, Hyper-Threading, 16 MB of L2 cache and 22 MB of L3 cache, and Turbo Boost to crank the clock speed over 4 GHz when the system needs it. I get shivers just thinking about it!
Integrated Memory Controller
All current microprocessors have an integrated memory controller IMC), moved from the motherboard chip into the CPU to optimize the flow of information into and out from the CPU. An IMC enables faster control over things like the large L3 cache shared among multiple cores.
Just like in so many other areas of computing, manufacturers implement a variety of IMC's in their CPUs. In practice, this means that different CPUs handle different types and capacities of RAM. I'll save the details on those RAM variations for later. For now, add "different RAM support" to your list of things to look at when making a CPU recommendation for a client.
Integrated Graphics Processing Unit
As to be discussed in later topics in "Display Technologies", the video processing portion of the computer--made up of the parts that put a changing image on the monitor--traditionally has a discrete microprocessor that differs in both function and architecture from the CPUs designed for general-purpose computing. The generic term for the video processor is graphic processing unit (GPU). I'll spare you the detail until we got to the chapter about video later on, but it turns out that graphics processors can handle certain tasks much more efficiently than the standard CPU. Integrating a GPU into a CPU enhances the overall performance of the computer while at the same time reducing energy use, size, and cost. With the proliferation of mobile devices and portable computers today, all these benefits have obvious merit.
Both Intel and AMD produce CPUs with integrated GPUs. For many years, the quality of the GPU performance with demanding graphical programs like games made the choice between the two easy. The Intel HD Graphics and Intel Iris Pro Graphics integrated into many Core i3/i5/i7 processors pale in comparison with their AMD accelerated processing unit (APU), such as the AMD A10. AMD bought one of the two dedicated GPU manufacturers--ATI--years ago and used their technology for microprocessors with integrated CPU and GPU. (The Xbox One and PlayStation 4 gaming systems, for example, use AMD APUs.) Intel is slowly closing the gap.
Security
All modern processors employ the NX bit technology that enables the CPU to protect certain sections of memory. This feature, coupled
Everybody calls the NX bit technology something different (but you don't need to memorize any of this for the exams):
- Intel: XD bit (eXecute Disable)
- AMD: Enhanced Virus Protection
- ARM: XN (eXecute Never)